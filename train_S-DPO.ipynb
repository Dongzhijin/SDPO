{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDPO Training Script with Reference Model Configuration\n",
    "# This script demonstrates how to configure SDPO with different reference model strategies\n",
    "\n",
    "import time, os\n",
    "# time.sleep(60*60*1)\n",
    "\n",
    "# =============================================================================\n",
    "# SDPO Configuration: Reference Model Selection\n",
    "# =============================================================================\n",
    "\n",
    "# Training configuration\n",
    "node_name = 'model3b-dpo0117-distill-33b-sdpo-beta002-lambda10'\n",
    "base_model_path = './models/model_3b'  # Policy model (3B parameters)\n",
    "\n",
    "# Reference Model Selection (Critical for SDPO effectiveness)\n",
    "# Choose one of the following strategies:\n",
    "\n",
    "# Strategy 1: Same-size DPO-aligned reference (computationally efficient)\n",
    "# ref_model_path = './models/model_3b-dpo-baseline'\n",
    "\n",
    "# Strategy 2: Larger reference model (better performance, higher cost)\n",
    "ref_model_path = './models/model_33b-dpo-baseline'  # 33B reference for 3B policy\n",
    "\n",
    "# Strategy 3: Use base model as reference (baseline comparison)\n",
    "# ref_model_path = base_model_path\n",
    "\n",
    "# =============================================================================\n",
    "# SDPO-specific Parameters\n",
    "# =============================================================================\n",
    "\n",
    "loss_type = 'sdpo'          # Enable Selective DPO\n",
    "threshold = 0.6             # Keep top 40% important tokens (SDPO threshold)\n",
    "lambda_sdpo = 10            # SDPO regularization parameter\n",
    "\n",
    "# Standard DPO parameters\n",
    "lr = 2e-7                   # Learning rate (slightly lower for SDPO)\n",
    "beta = 0.01                 # DPO beta parameter\n",
    "\n",
    "# Hardware configuration\n",
    "work_num = 4\n",
    "gpu_pool = ''\n",
    "\n",
    "# Data configuration\n",
    "train_file = '/data/Skywork-Reward-Preference-80K-v0.2'\n",
    "\n",
    "# Output configuration\n",
    "PROJ_PATH_BOLE = f'./'\n",
    "OUTPUT_DIR = os.path.join('./DPO_output', node_name)\n",
    "BATCH_SIZE = 1280 // (work_num * 8)\n",
    "\n",
    "# =============================================================================\n",
    "# Training Script Construction\n",
    "# =============================================================================\n",
    "\n",
    "train_script = f'examples/scripts/dpo.py \\\n",
    "    --deepspeed /code/dongzhijin/trl/trl-main/ds_config/ds_config_zero3_dzj.json \\\n",
    "    --dataset_name {train_file} \\\n",
    "    --model_name_or_path {base_model_path} \\\n",
    "    --ref_model_name_or_path {ref_model_path} \\\n",
    "    --learning_rate {lr} \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps {BATCH_SIZE} \\\n",
    "    --gradient_checkpointing \\\n",
    "    --logging_steps 2 \\\n",
    "    --eval_strategy no \\\n",
    "    --eval_steps 4 \\\n",
    "    --save_steps 20 \\\n",
    "    --save_total_limit 100 \\\n",
    "    --output_dir {OUTPUT_DIR} \\\n",
    "    --no_remove_unused_columns \\\n",
    "    --max_length 4096 \\\n",
    "    --warmup_steps 30 \\\n",
    "    --loss_type {loss_type} \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --beta {beta} \\\n",
    "    --bf16 True \\\n",
    "    --threshold {threshold} \\\n",
    "    --lambda_sdpo {lambda_sdpo} \\\n",
    "    '\n",
    "\n",
    "print(f\"\"\"\n",
    "=============================================================================\n",
    "SDPO Training Configuration Summary\n",
    "=============================================================================\n",
    "Policy Model:     {base_model_path}\n",
    "Reference Model:  {ref_model_path}\n",
    "Loss Type:        {loss_type}\n",
    "SDPO Threshold:   {threshold} (keeps top {int((1-threshold)*100)}% important tokens)\n",
    "Beta:             {beta}\n",
    "Lambda SDPO:      {lambda_sdpo}\n",
    "Learning Rate:    {lr}\n",
    "Output Dir:       {OUTPUT_DIR}\n",
    "=============================================================================\n",
    "\"\"\")\n",
    "\n",
    "# Launch training\n",
    "! deepspeed --num_gpus {work_num} {train_script}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
